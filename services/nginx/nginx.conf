events {
    worker_connections 1024;
}

http {
    upstream vllm_service {
        server llamacpp:8000;
    }
    
    upstream tts_service {
        server text-to-speech:6005;
    }
    
    upstream stt_service {
        server speech-to-text:6001;
    }
    
    upstream agent_service {
        server agent:6002;
    }

    upstream agent_service_openai {
        server agent:6006;
    }
    
    # TTS UI
    upstream tts_ui {
        server text-to-speech:6004;
    }
    
    server {
        listen 80;
        
        location /health {
            access_log off;
            return 200 "AI Gateway healthy\n";
            add_header Content-Type text/plain;
        }
        
        # Agent LLM endpoint (via Agent- NOT real OpenAI-compat
        # Accept OpenAI-compat request but only pulls latest message...hacky)
        location /v1/chat/completions {
            proxy_pass http://agent_service_openai;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_read_timeout 300s;
            proxy_send_timeout 300s;
        }

        # vLLM direct connect, no Agent but full OpenAI-compat)
        location /v2/chat/completions {
            proxy_pass http://vllm_service;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_read_timeout 300s;
            proxy_send_timeout 300s;
        }
        
        location /v2/completions {
            proxy_pass http://vllm_service;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_read_timeout 300s;
            proxy_send_timeout 300s;
        }
        # TODO: Serve v1 and v2? 
        location /v1/models {
            proxy_pass http://vllm_service;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
        
        # TTS endpoint
        location /v1/audio/speech {
            proxy_pass http://tts_service;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_read_timeout 300s;
            proxy_send_timeout 300s;
            client_max_body_size 10M;
        }
        
        # STT endpoints
        location /v1/audio/transcriptions {
            proxy_pass http://stt_service;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_read_timeout 300s;
            proxy_send_timeout 300s;
            client_max_body_size 25M;
        }
        
        location /v1/audio/translations {
            proxy_pass http://stt_service;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_read_timeout 300s;
            proxy_send_timeout 300s;
            client_max_body_size 25M;
        }
        
        location /agent/ {
            proxy_pass http://agent_service/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
        
        # TTS UI
        location /tts-ui/ {
            proxy_pass http://tts_ui/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
        
        location / {
            return 200 '{"message": "AI Gateway", "services": {"llm": "/v1/chat/completions", "tts": "/v1/audio/speech", "stt": "/v1/audio/transcriptions", "agent": "/agent/", "tts_ui": "/tts-ui/"}}';
            add_header Content-Type application/json;
        }
    }
}