services:
  postgres:
    image: postgres:15-alpine
    env_file:
      - .env
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./services/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U havencore -d havencore"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./services/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - agent
      - text-to-speech
      - speech-to-text
  agent:
    build:
      context: ./services/agent
      dockerfile: Dockerfile
    image: agent:${VERSION:-latest}
    env_file:
      - .env
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6002/"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 15s
    volumes:
      - ./services/agent/:/app
      # - ./shared:/app/shared:ro
    ports:
      - "6002:6002"
      - "6006:6006" # /v1/chat/completions OpenAI-compat API
    depends_on:
      - postgres
    restart: unless-stopped
  speech-to-text:
    build:
      context: ./services/speech-to-text
      dockerfile: Dockerfile
    image: speech-to-text:${VERSION:-latest}
    volumes:
      - ./services/speech-to-text/app:/app
      - ./shared:/app/shared:ro
    ports:
      - "6001:6001" # /v1/audio/transcriptions OpenAI-compat API
      - "6000:6000"
      - "5999:5999"
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
  text-to-speech:
    build:
      context: ./services/text-to-speech
      dockerfile: Dockerfile
    image: text-to-speech:${VERSION:-latest}
    ports:
      - "6005:6005" # OpenAI API-compatible /v1/audio/speech
      - "6004:6004" # UI/API
      - "6003:6003" # Serving audio files
    env_file:
      - .env
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6004/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        reservations: # GPU Device to use is configured in shared_config.py
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./services/text-to-speech/app/output:/app/output
      - ./services/text-to-speech/app/config.py:/app/config.py
      - ./shared:/app/shared:ro
    restart: unless-stopped
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    # profiles: ["vllm"]
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./services/vllm/app:/app
    ipc: host
    # command: --model TechxGenus/Mistral-Large-Instruct-2411-AWQ --served-model-name gpt-3.5-turbo -tp 4 --max-model-len 16384 --port 8000 --enable-auto-tool-choice --tool-call-parser mistral --chat-template /app/mistral-tool-chat-template.jinja --gpu-memory-utilization .74
    command: >
      --model Qwen/Qwen2.5-72B-Instruct-AWQ
      --served-model-name gpt-3.5-turbo
      --quantization awq_marlin
      --max-num-seqs 1
      --enforce-eager
      -tp 2
      --max-model-len 16384
      --port 8000
      --enable-auto-tool-choice
      --tool-call-parser hermes
      --gpu-memory-utilization .95
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
  iav-to-text:
      build:
        context: ./services/iav-to-text
        dockerfile: Dockerfile
      runtime: nvidia
      environment:
        - CUDA_VISIBLE_DEVICES=2
      ports:
        - "8100:8100"  # vLLM API port
        - "8110:8110"  # Gradio UI port
      volumes:
        - ~/.cache/huggingface:/root/.cache/huggingface
        - ./services/iav-to-text/app:/app
      ipc: host
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8100/v1/models"]
        interval: 15s
        timeout: 5s
        retries: 5
        start_period: 300s
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: all
                capabilities: [gpu]
      restart: unless-stopped
  mosquitto:
    image: eclipse-mosquitto:latest
    container_name: mosquitto
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "1883:1883"    # MQTT port
      - "9001:9001"    # Websocket port (optional, useful for web clients)
    volumes:
      - ./services/mosquitto/config:/mosquitto/config
      - ./services/mosquitto/data:/mosquitto/data
      - ./services/mosquitto/log:/mosquitto/log
    # healthcheck:
    #   test: ["CMD", "mosquitto_sub", "-t", "$$SYS/#", "-C", "1", "-i", "healthcheck", "-W", "3"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3

  # llamacpp:
  #   image: ghcr.io/ggml-org/llama.cpp:server-cuda-backup-20250816
  #   volumes:
  #     - ./services/llamacpp/models:/models
  #   # profiles: ["llamacpp"]
  #   ports:
  #     - "8000:8000"
  #   command: >
  #     -m /models/Qwen2.5-72B-Instruct-Q6_K/Qwen2.5-72B-Instruct-Q6_K-00001-of-00002.gguf
  #     -dev CUDA0,CUDA1,CUDA2
  #     --alias gpt-3.5-turbo
  #     -md /models/Qwen2.5-7B-Instruct-Q6_K_L.gguf
  #     -devd CUDA3
  #     --draft-max 64
  #     --batch-size 2048
  #     --ubatch-size 512
  #     -fa
  #     -sm layer
  #     --ctx-size 8192
  #     --ctx-size-draft 2048
  #     -ngl 99
  #     -ngld 99
  #     --no-context-shift
  #     --port 8000
  #     --jinja
  #     --api-key 1234123412341234
  #     --host 0.0.0.0
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 15s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 300s
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC (optional)
    volumes:
      - ./volumes/qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__LOG_LEVEL=INFO
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
  embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:1.8
    ports:
      - "3000:3000"
    volumes:
      - ./volumes/models:/data
    environment:
      - MODEL_ID=BAAI/bge-large-en-v1.5
      - PORT=3000
      - CUDA_VISIBLE_DEVICES=3  # Set to the GPU index you want to use (e.g., 3)
    deploy:
      resources:
        reservations: # GPU Device to use is configured in shared_config.py
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  postgres_data:
