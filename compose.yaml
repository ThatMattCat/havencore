services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./services/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - agent
      - text-to-speech
      - speech-to-text
      - vllm
  agent:
    build:
      context: ./services/agent
      dockerfile: Dockerfile
    image: agent:${VERSION:-latest}
    env_file:
      - .env
    depends_on:
      vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6002/"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 15s
    volumes:
      - ./services/agent/app:/app
      - ./shared:/app/shared:ro
    ports:
      - "6002:6002"
    restart: unless-stopped
  speech-to-text:
    build:
      context: ./services/speech-to-text
      dockerfile: Dockerfile
    image: speech-to-text:${VERSION:-latest}
    volumes:
      - ./services/speech-to-text/app:/app
      - ./shared:/app/shared:ro
    ports:
      - "6001:6001" # /v1/audio/transcriptions OpenAI-compat API
      - "6000:6000"
      - "5999:5999"
    environment:
      - CUDA_VISIBLE_DEVICES=3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
  text-to-speech:
    build:
      context: ./services/text-to-speech
      dockerfile: Dockerfile
    image: text-to-speech:${VERSION:-latest}
    ports:
      - "6005:6005" # OpenAI API-compatible /v1/audio/speech
      - "6004:6004" # UI/API
      - "6003:6003" # Serving audio files
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6004/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    environment:
      - COQUI_TOS_AGREED=1
    deploy:
      resources:
        reservations: # GPU Device to use is configured in shared_config.py
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:  # TODO: why did I use individual volumes?
      - ./services/text-to-speech/app/flagged:/app/flagged
      - ./services/text-to-speech/app/output:/app/output
      - ./services/text-to-speech/app/config.py:/app/config.py
      - ./shared:/app/shared:ro
    restart: unless-stopped
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./services/vllm/app:/app
    ipc: host
    command: --model TechxGenus/Mistral-Large-Instruct-2411-AWQ --served-model-name gpt-3.5-turbo -tp 4 --max-model-len 16384 --port 8000 --enable-auto-tool-choice --tool-call-parser mistral --chat-template /app/mistral-tool-chat-template.jinja --gpu-memory-utilization .74
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped