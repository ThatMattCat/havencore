services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: havencore
      POSTGRES_USER: havencore
      POSTGRES_PASSWORD: havencore_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./services/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U havencore -d havencore"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./services/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - agent
      - text-to-speech
      - speech-to-text
  agent:
    build:
      context: ./services/agent
      dockerfile: Dockerfile
    image: agent:${VERSION:-latest}
    env_file:
      - .env
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6002/"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 15s
    volumes:
      - ./services/agent/app:/app
      - ./shared:/app/shared:ro
    ports:
      - "6002:6002"
      - "6006:6006" # /v1/chat/completions OpenAI-compat API
    depends_on:
      - postgres
    restart: unless-stopped
  speech-to-text:
    build:
      context: ./services/speech-to-text
      dockerfile: Dockerfile
    image: speech-to-text:${VERSION:-latest}
    volumes:
      - ./services/speech-to-text/app:/app
      - ./shared:/app/shared:ro
    ports:
      - "6001:6001" # /v1/audio/transcriptions OpenAI-compat API
      - "6000:6000"
      - "5999:5999"
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
  text-to-speech:
    build:
      context: ./services/text-to-speech
      dockerfile: Dockerfile
    image: text-to-speech:${VERSION:-latest}
    ports:
      - "6005:6005" # OpenAI API-compatible /v1/audio/speech
      - "6004:6004" # UI/API
      - "6003:6003" # Serving audio files
    env_file:
      - .env
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6004/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        reservations: # GPU Device to use is configured in shared_config.py
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./services/text-to-speech/app/output:/app/output
      - ./services/text-to-speech/app/config.py:/app/config.py
      - ./shared:/app/shared:ro
    restart: unless-stopped
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    # profiles: ["vllm"]
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./services/vllm/app:/app
    ipc: host
    # command: --model TechxGenus/Mistral-Large-Instruct-2411-AWQ --served-model-name gpt-3.5-turbo -tp 4 --max-model-len 16384 --port 8000 --enable-auto-tool-choice --tool-call-parser mistral --chat-template /app/mistral-tool-chat-template.jinja --gpu-memory-utilization .74
    command: >
      --model TechxGenus/Mistral-Large-Instruct-2411-AWQ
      --served-model-name gpt-3.5-turbo
      -tp 4
      --max-model-len 16384
      --port 8000
      --enable-auto-tool-choice
      --tool-call-parser mistral
      --chat-template /app/mistral-tool-chat-template.jinja
      --gpu-memory-utilization .74
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
  llamacpp:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda-backup-20250816
    volumes:
      - ./services/llamacpp/models:/models
    # profiles: ["llamacpp"]
    ports:
      - "8000:8000"
    command: >
      -m /models/Qwen2.5-72B-Instruct-Q6_K/Qwen2.5-72B-Instruct-Q6_K-00001-of-00002.gguf
      -dev CUDA0,CUDA1,CUDA2
      --alias gpt-3.5-turbo
      -md /models/Qwen2.5-7B-Instruct-Q6_K_L.gguf
      -devd CUDA3
      --draft-max 64
      --batch-size 2048
      --ubatch-size 512
      -fa
      -sm layer
      --ctx-size 8192
      --ctx-size-draft 2048
      -ngl 99
      -ngld 99
      --no-context-shift
      --port 8000
      --jinja
      --api-key 1234123412341234
      --host 0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

      # -m /models/Qwen3-235B-A22B-Instruct-2507-GGUF/UD-Q4_K_XL/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf
      # --flash-attn
      # --threads 32
      # --ctx-size 16384
      # --temp 0.1
      # --min-p 0.0
      # --top-p 0.8
      # --top-k 20
      # --presence-penalty 0.5
      # -ngl 99
      # --no-context-shift
      # -ot "([5-9]+).ffn_.*_exps.=CPU"
      # --port 8000
      # -ts 12,16,16,14
      # --jinja
      # --api-key 1234123412341234
      # --host 0.0.0.0
  speaches:
    container_name: speaches
    image: ghcr.io/speaches-ai/speaches:latest-cuda-12.6.3
    build:
      dockerfile: Dockerfile
      context: ./services/speaches
      platforms:
        - linux/amd64
      args:
        BASE_IMAGE: nvidia/cuda:12.9.0-cudnn-runtime-ubuntu24.04
    restart: unless-stopped
    ports:
      - 8000:6007
    volumes:
      - ~/.cache/huggingface/hub:/home/ubuntu/.cache/huggingface/hub
    develop:
      watch:
        - action: rebuild
          path: ./uv.lock
        - action: sync+restart
          path: ./src
          target: /home/ubuntu/speaches/src
    env_file:
      - path: .env
        required: false
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://0.0.0.0:8000/health"] # TODO: won't work if a user changes the port
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
volumes:
  postgres_data:
